{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e00c5057-9c16-4b30-b0d4-b636807e66e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The objective of this notebook is to build a baseline machine learning model for deqliquncy variables \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0805268-d181-4a03-b9a3-3989f641f9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40314bb8-7ca2-4d14-8551-b6bfd944259d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1.1 Import Relevant Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc4b3fd-e513-4266-8b61-60788cc0538b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ignore SettingWithCopyWarning\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a079917-058a-4056-bb00-6eac99b510e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Loading the datasets and basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92211855-50cd-439c-973f-39c4588954e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2.1 Loading tables as pyspark dataframes"
    }
   },
   "outputs": [],
   "source": [
    "appl_train = spark.sql(\"SELECT * FROM default.application_train\")\n",
    "# bu_features1 = spark.sql(\"SELECT * FROM default.bu_features_level1\")\n",
    "# bu_features2 = spark.sql(\"SELECT * FROM default.bu_features_level2\")\n",
    "bu_features_final2 = spark.sql(\"SELECT * FROM default.bu_features_final2\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa0d5d9-f824-4796-b57e-a122846cf8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "appl_train.createOrReplaceTempView(\"appl_train\")\n",
    "# bu_features1.createOrReplaceTempView(\"bu_features1\")\n",
    "# bu_features2.createOrReplaceTempView(\"bu_features2\")\n",
    "bu_features_final2.createOrReplaceTempView(\"bu_features_final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fad053-9508-407e-b26c-33659f85ad45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(distinct SK_ID_CURR) AS UNIQUE_CUSTOMER_COUNT, count(*) as row_count FROM appl_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c107b05a-88a4-450a-96b1-de83e137fd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(distinct SK_ID_CURR) AS UNIQUE_CUSTOMER_COUNT, count(*) as row_count FROM bu_features_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4758a38-ebb3-4441-b1c8-584427e734b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "appl_train.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb8598d-ae41-4ff8-b3d1-9c30fbd05cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(distinct SK_ID_CURR) AS UNIQUE_CUSTOMER_COUNT, count(*) as row_count from bu_features_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958b3ed9-dbe7-4cde-a2e5-44cd132dd48a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2.3 Left joining this with appl_train"
    }
   },
   "outputs": [],
   "source": [
    "# We want to map the targer values against the corresponding SK_ID_CURR from appl_train\n",
    "# Hence we are Left joining and keeping only the target value\n",
    "deq_model_train_data_1 = bu_features_final2.join(appl_train.select(\"SK_ID_CURR\", \"TARGET\"), \n",
    "                          on=\"SK_ID_CURR\", \n",
    "                          how=\"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bf3f67-929b-4df7-bed9-ca220ba76857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deq_model_train_data_1.limit(2).display()\n",
    "deq_model_train_data_1.createOrReplaceTempView(\"deq_model_train_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eeac2f9-a815-4914-af28-af3e49b17da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(distinct SK_ID_CURR) AS UNIQUE_CUSTOMER_COUNT, count(*) as row_count from deq_model_train_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2d2fd0-8b87-41ff-90dc-c77dbca7bed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now our base dataframe for training is ready and we are going to store it as a table in the hivestore catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd4d85e-0509-4475-850d-7e071bb5e946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists default.deq_model_train_data_1;\n",
    "create table default.deq_model_train_data_1 as\n",
    "select\n",
    "  *\n",
    "from\n",
    "  deq_model_train_data_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "758c5cbf-b19a-4478-b432-c56bbde6eebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Model Training Lifecycle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df02b6c4-719d-4ccd-935f-82339b0cf373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15135e78-29c7-4001-ab74-eed8c116cf70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f92a3f6-e76c-4d11-9553-9a989bd22daf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbe8adc-134f-446e-a3cb-dada6731920a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the maximum number of rows to display\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8356b14-8bd7-4184-a555-15aff6e06fc2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading the Dataset"
    }
   },
   "outputs": [],
   "source": [
    "deq_model_train_data_1 = spark.sql(\"SELECT * FROM default.deq_model_train_data_1\")\n",
    "deq_model_train_data_1.createOrReplaceTempView(\"deq_model_train_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f3fbb1-1495-44ed-8710-5b03f3a4b1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(distinct SK_ID_CURR) as UNIQUE_CUSTOMER_COUNT, count(*) as row_count from deq_model_train_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13be5d3-efc8-4d49-9a25-3999f2fdd5c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Converting the pyspark df to pandas df"
    }
   },
   "outputs": [],
   "source": [
    "data_raw = deq_model_train_data_1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1e3889-c6ab-4ae8-a935-e94c5dad6376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bfb8cf-8a8a-4891-b281-f85f53128e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(data_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1786e302-ac27-473f-8550-1234efd4b5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(data_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd93fb1e-2f8c-4da1-8bd3-1a82935a274c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculating fillrate"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate fill rate for each column\n",
    "fill_rate = data_raw.notnull().mean() * 100\n",
    "\n",
    "# Display the fill rate as a DataFrame\n",
    "fill_rate_df = fill_rate.reset_index()\n",
    "fill_rate_df.columns = ['Column', 'Fill Rate (%)']\n",
    "print(fill_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c825839b-3a44-4eb6-9d3b-851b587ea3a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dropping Columns with less than 50% fillrate"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate fill rate\n",
    "fill_rate = data_raw.notnull().mean() * 100\n",
    "\n",
    "# Filter and keep only columns with fill rate >= 20%\n",
    "data_1 = data_raw.loc[:, fill_rate >= 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ee1855-3d8e-48be-b75b-87054caadb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adfe5ef-a602-4324-8213-4258d2843375",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "target variable distribution"
    }
   },
   "outputs": [],
   "source": [
    "target_counts = data_1[\"TARGET\"].value_counts()\n",
    "print(target_counts)\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(\n",
    "    target_counts,\n",
    "    labels=target_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[\"skyblue\", \"orange\"],\n",
    "    startangle=90,\n",
    "    explode=(0.05, 0.05),  # Slightly separate the slices for better visibility\n",
    ")\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9b7fee7-4d23-4bb3-b0da-b8fa817de46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Clearly the TARGET Variable distribution is highly imabalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df115553-1892-4840-abe8-728a3d1075ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8730a4f-da59-4348-b540-5b630c782419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since I plan to use a neural network for this problem, we will have to handle null values. This is a finanical dataset and null values have meaning (example no loan taken etc). Hence we cannot just replace it with measures of central tendancy or build some model to impute values. Instead we will use some absurd number like -999 etc as placeholders. Ideally I should figure out categories inside the null values  - represting different reasons for the null values and assign each of them separate placeholders. But for the sake of simplicity and also because I have less time, I am just gonna replace all null values with -999 and add an indicator variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f203313-9d11-4d4f-8db9-70f07828ccc7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Mean Value Imputation with placeholder and indicators"
    }
   },
   "outputs": [],
   "source": [
    "# Replace nulls with a placeholder (-999)\n",
    "data_cleaned = data_1.copy()\n",
    "placeholder_value = -999\n",
    "data_cleaned = data_cleaned.fillna(placeholder_value)\n",
    "\n",
    "# # Add missing value indicators for all features with nulls\n",
    "# for col in data_1.columns:\n",
    "#     if data_1[col].isnull().any():\n",
    "#         data_cleaned[f\"{col}_isnull\"] = data_1[col].isnull().astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092da3f2-020a-491a-b6ad-a2dac9be14e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc1f58c-fccb-4964-8849-d3a5d7a810f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b7c863-5edc-4b52-90e5-0fa7fe2232e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Splitting into X and y"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting features and target\n",
    "X = data_cleaned.drop(columns=[\"TARGET\",\"SK_ID_CURR\"])\n",
    "y = data_cleaned[\"TARGET\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88689ec-93cd-4570-b82a-35e130090425",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Standardization"
    }
   },
   "outputs": [],
   "source": [
    "# Identify indicator columns (ending in '_isnull')\n",
    "indicator_columns = [col for col in X.columns if col.endswith('_isnull')]\n",
    "non_indicator_columns = [col for col in X.columns if col not in indicator_columns]\n",
    "\n",
    "# Scale only non-indicator columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[non_indicator_columns] = scaler.fit_transform(X[non_indicator_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c238eb-a984-4bee-b76f-c1cee6c25e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19003b14-fcfd-45d5-83d6-4660075047b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0458add-27f6-44b7-9d01-d1bb198931dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Getting an idea of feature importances"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fit a Random Forest to rank features\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaled, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": X_scaled.columns,\n",
    "    \"Importance\": rf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ce0d8d-eba0-4f70-8b9e-190649ce7bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f4ffd4d-9177-4297-9775-73f4e3e4d821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Not making a decision based on the above. Choosing experimemt structure 2.1.2 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1adfddfe-c369-4cb8-a816-6808d92131f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train test split"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22181be1-3155-4c24-8b5e-74cbfc9e1c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed41c700-fdb2-43c5-815e-147b701373d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.3 Building Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e5a35e0-fd54-4a4a-a6fe-0a63c30648d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "experiment 1 NN"
    }
   },
   "outputs": [],
   "source": [
    "# # Define the neural network model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Input layer (155 features)\n",
    "# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # 128 neurons in the first layer\n",
    "\n",
    "# # Hidden layers\n",
    "# model.add(Dense(64, activation='relu'))  # Second hidden layer with 64 neurons\n",
    "# model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
    "\n",
    "# model.add(Dense(32, activation='relu'))  # Third hidden layer with 32 neurons\n",
    "# model.add(Dropout(0.5))  # Dropout\n",
    "\n",
    "# # Output layer (binary classification, single output unit)\n",
    "# model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a6236b-a762-4a0e-ad1e-50f990d1dc44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "experiment 2 NN"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input layer (256 features)\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))  # First hidden layer with 256 neurons\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(128, activation='relu'))  # Second hidden layer with 128 neurons\n",
    "model.add(Dropout(0.6))  # Dropout to prevent overfitting\n",
    "\n",
    "model.add(Dense(64, activation='relu'))  # Third hidden layer with 64 neurons\n",
    "model.add(Dropout(0.6))  # Dropout\n",
    "\n",
    "# Output layer (binary classification, single output unit)\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification \n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "279a2fd9-fc9e-410e-8836-fb955aee85e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate class weights for the imbalance\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Compile the model with binary cross-entropy loss\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d0cb3d-8161-471d-aa39-07ec2f900164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if class weights are calculated correctly\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a755865-387e-4758-bccb-3e1b455894c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bceb1c-35a1-4844-b3a5-ff29ea60f369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    class_weight=class_weight_dict,  # Apply class weights here\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0825d019-5d27-4be9-80da-5116315524ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7f1dab-bba3-4f16-9697-532c0ecdeaec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5)  # Convert probabilities to class labels (0 or 1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23047760-b42f-44df-926d-b13fed7068bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a directory for your model\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/my_model_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82fe7f1-208f-4342-bd85-65c567bbdb7d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Saving the model"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save model to the driver node local disk\n",
    "local_path = \"/tmp/ann_model_exp1.keras\"\n",
    "model.save(local_path)\n",
    "\n",
    "# Copy the model from the local disk to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", \"dbfs:/FileStore/my_model_directory/ann_model_exp1.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a0561d-3b64-4368-aa91-1b95a893ae36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Predicting using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f10bcc-7773-442d-a385-c321e10e7247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep a copy of SK_ID_CURR before resetting the index\n",
    "X_scaled[\"SK_ID_CURR\"] = data_cleaned[\"SK_ID_CURR\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762919e5-08ae-43f9-8091-2965dddf74dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4306c7e4-9bef-46e4-8ae8-3d8f514f82fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.predict(X_scaled.drop(columns=[\"SK_ID_CURR\"]))  # Drop SK_ID_CURR if it was re-added\n",
    "predicted_probabilities = predictions.flatten()\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "prediction_df = pd.DataFrame({\n",
    "    \"SK_ID_CURR\": X_scaled[\"SK_ID_CURR\"],  # Use the retained SK_ID_CURR\n",
    "    \"Prediction_Probability\": predicted_probabilities\n",
    "})\n",
    "\n",
    "final_data_with_predictions = data_cleaned.merge(prediction_df, on=\"SK_ID_CURR\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5686180e-cff6-4cec-99c5-23164bc55607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac687aec-5a44-48ae-aa80-4b00b3a717c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bffe23d-4999-408e-9279-119164a6299c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#converting to pyspark df\n",
    "bu_final_pred_df = spark.createDataFrame(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921208cc-93d6-4a2e-bb8c-d703393a3c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bu_final_pred_df.createOrReplaceTempView(\"bu_final_pred_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154cdeff-5297-44b5-bcf3-c921dedf9e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists default.bu_final_prediction_data;\n",
    "create table default.bu_final_prediction_data as\n",
    "select\n",
    "  *\n",
    "from\n",
    "  bu_final_pred_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b88ff9c-a30f-414d-8a02-aabb35094219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating Final Residual **DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3356d05d-038d-4739-a2cb-3e50de6e238c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa70ba42-a13d-409b-a0fa-1c70e124d16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deq_model_train_data_1 = spark.sql(\"SELECT * FROM default.deq_model_train_data_1\")\n",
    "df_pred = spark.sql(\"SELECT * FROM default.bu_final_prediction_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d1f8ab-4c74-4181-a236-4b4f96f4891a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform inner join on the common column\n",
    "joined_df = df_pred.join(deq_model_train_data_1, on=\"SK_ID_CURR\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e766b1d-3fc3-49e3-a4eb-077cd69fe02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame with SK_ID_CURR, predicted_prob, and residual\n",
    "\n",
    "final_df = joined_df.select(\n",
    "    F.col(\"SK_ID_CURR\"),\n",
    "    F.col(\"Prediction_Probability\"),\n",
    "    (F.col(\"TARGET\") - F.col(\"Prediction_Probability\")).alias(\"residual\"),\n",
    "    F.col(\"b_DAYS_CREDIT_mean\"),\n",
    "    F.col(\"b_720_DAYS_CREDIT_PLAN_sum\"),\n",
    "    F.col(\"b_720_AMT_CREDIT_MAX_OVERDUE_sum\"),\n",
    "    F.col(\"b_consumer_DAYS_CREDIT_ENDDATE_mean\"),\n",
    "    F.col(\"b_credit_AMT_CREDIT_SUM_DEBT_sum\"),\n",
    "    F.col(\"b_CNT_CREDIT_PROLONG_sum\"),\n",
    "    F.col(\"b_365_AMT_CREDIT_SUM_OVERDUE_sum\"),\n",
    "    F.col(\"b_credit_AMT_CREDIT_DEBT_DIFF_mean\"),\n",
    "    F.col(\"b_credit_DAYS_CREDIT_mean\"),\n",
    "    F.col(\"b_active_DAYS_CREDIT_mean\"),\n",
    "    F.col(\"DEQ_AVG_COUNT_DPD0P_36MOB_ALL\"),\n",
    "    F.col(\"DEQ_AVG_COUNT_DPD0P_3MOB_ALL\")\n",
    ")\n",
    "\n",
    "# Show the result for verification\n",
    "final_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac5c2eb-140a-4fae-92dc-485daa2deeea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.createOrReplaceTempView(\"final_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f90123-3466-4072-b6b8-833124a66cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists default.bu_final_prediction_data2;\n",
    "create table default.bu_final_prediction_data2 as\n",
    "select\n",
    "  *\n",
    "from\n",
    "  final_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5592a4f3-e0c6-4476-b9f4-c6217bf98620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778db41b-68d5-49dd-af01-6f11c1c59be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Replace spaces with underscores in all column names\n",
    "for col in pyspark_df.columns:\n",
    "    new_col = col.replace(\" \", \"_\")\n",
    "    pyspark_df = pyspark_df.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "display(pyspark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b926e6a-8a0a-4e59-b9d8-1ddee97ea24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2371779100834191,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SM_Deliquency_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
